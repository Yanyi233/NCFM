# 分布式训练配置 (如果使用)
distibution_train:
  backend: 'nccl' # choices=['nccl', 'gloo', 'mpi', 'torch']
  init_method: 'env://'
  workers: 8      # 根据你的硬件调整 DataLoader workers

# 数据集配置
dataset:
  dataset: reuters
  nclass: 90                # VOC2007 类别数 (会被动态覆盖)
  # size: 224                 # 图像尺寸 (会被动态覆盖)
  data_dir: '../dataset/reuters'
  load_memory: True        
  batch_real: 256            # Condense 模式下的真实数据批次大小
  # nch: 3                    # 图像通道数
  is_multilabel: True       # 显式声明为多标签

# 网络配置
network:
  model_path: '../models/model/bert-base-uncased'
  net_type: BERT         # 使用resnet18
  norm_type: layernorm          # 使用 batch (或 instance)
  depth: 12                 
  layer_index: [3, 6, 9, 12]
  max_length: 512
  # width: 1.0                # 在resnet18中，不使用该参数

# 训练配置 (用于 Pretrain 和 Evaluation)
train:
  pre_eval_epochs: 10        # 蒸馏前预热评估的训练轮数
  evaluation_epochs: 400     # 蒸馏过程中的评估轮数
  epoch_print_freq: 10      # 打印频率
  epoch_eval_interval: 20    # 评估间隔 (轮)
  pertrain_epochs: 30       # 预训练模式下的轮数 (根据需要调整)
  batch_size: 32            # 预训练/评估时的训练批次大小 (会被 world_size 除)
  lr: 0.001               # 学习率 (BERT 微调推荐范围)
  adamw_lr: 0.0008         # 如果 eval_optimizer 是 adamw
  eval_optimizer: adamw      # 评估时使用的优化器 (adam 或 sgd)
  momentum: 0.9             # SGD momentum (如果 eval_optimizer 是 sgd)  
  weight_decay: 0.01        # 权重衰减 (BERT 微调常用设定)
  seed: 42                  # 随机种子
  model_num: 10             # 预训练模型的数量 (如果需要生成多个)
  warmup_steps: 100         # 学习率预热步数
  pred_threshold: 0.3       # 多标签预测阈值
  grad_clip_norm: 1.0       # 梯度裁剪阈值

# 数据增强配置
augmentation:
  mixup: cut               # 不使用 mixup/cutmix (none, cut, mix)
  beta: 1.0                 # Mixup beta (如果 mixup != none)
  mix_p: 0.5                # Mixup 概率 (如果 mixup != none)
  rrc: False                # 是否使用 RandomResizedCrop (通常用于 ImageNet)
  dsa: False                # 是否使用 DSA
  dsa_strategy: ""      # DSA 策略 (如果 dsa=true)
  aug_type: ''          # 基础增强类型 (none, color_crop_cutout, etc.)

# 优化配置 (用于 Condense)
optimization:
  optimizer: adamw           # 蒸馏优化器 (adam 或 sgd)
  lr_scale_adam: 0.1        # Adam/AdamW 学习率缩放因子 (相对于 lr_img)
  lr_img: 0.01              # 蒸馏图像的学习率 (如果 optimizer=sgd)
  mom_img: 0.5              # 蒸馏图像的 momentum (如果 optimizer=sgd)
  lr_sampling_net: 1e-3     # 采样网络学习率 (如果使用)

# 保存路径配置
save_path:
  save_dir: "../results/condense" # 结果保存目录 (建议包含数据集和ipc)
  pretrain_dir: '../pretrained_models/reuters/ipc10/bert_20251030' # 预训练模型保存目录

# 蒸馏 (Condense) 配置
condense:
  ipc: 10                   # 每个类别的图像数
  num_premodel: 10          # 使用的预训练模型数量 (应 <= train.model_num)
  niter: 400               # 蒸馏迭代次数 (outer_loop)
  iter_calib: 1             # 校准迭代次数
  calib_weight: 1           # 校准权重
  sampling_net: False       # 是否使用采样网络
  num_freqs: 4096           # NCFM 频率数量
  dis_metrics: "NCFM"       # 蒸馏度量
  factor: 2                 # NCFM 因子
  alpha_for_loss: 0.5       # NCFM 损失 alpha
  beta_for_loss: 0.5        # NCFM 损失 beta
  decode_type: 'single'     # 解码类型 (single, multi, bound)
  teacher_model_epoch: 20   # 使用哪个 epoch 的教师模型
